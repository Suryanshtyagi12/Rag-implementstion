{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "651628fa3d2c4de4bd51429b269f7a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3ed6790cc274b998946d7b1b17ed1b4",
              "IPY_MODEL_ae40a0ae54ab477ea5cde185efe2ca2f",
              "IPY_MODEL_69fc1fa2e5b94e358364ec59c4ff9bfa"
            ],
            "layout": "IPY_MODEL_c6089957b6a64aefa3a82fa57eea2096"
          }
        },
        "b3ed6790cc274b998946d7b1b17ed1b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3a841e335fe4c72a13efc47b3f99dcc",
            "placeholder": "​",
            "style": "IPY_MODEL_02e6771b144e45ad82a9f660ada4d97a",
            "value": "Batches: 100%"
          }
        },
        "ae40a0ae54ab477ea5cde185efe2ca2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_915cf7462dc64dd3a4d5e3239bec0cae",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f733f86a4f54725acbad212c754931a",
            "value": 1
          }
        },
        "69fc1fa2e5b94e358364ec59c4ff9bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6251eb0f45604b8bb88b4207fd96226a",
            "placeholder": "​",
            "style": "IPY_MODEL_e26b8658787043d783cd0a8065f3ae70",
            "value": " 1/1 [00:00&lt;00:00,  1.89it/s]"
          }
        },
        "c6089957b6a64aefa3a82fa57eea2096": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3a841e335fe4c72a13efc47b3f99dcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02e6771b144e45ad82a9f660ada4d97a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "915cf7462dc64dd3a4d5e3239bec0cae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f733f86a4f54725acbad212c754931a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6251eb0f45604b8bb88b4207fd96226a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e26b8658787043d783cd0a8065f3ae70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqK3uV0sBFHd"
      },
      "outputs": [],
      "source": [
        "#RAG Pipelined -Data Ingestion to vector DB Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-community langchain-openai faiss-cpu pypdf pymupdf tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_YTosTqBPO1",
        "outputId": "5d9dadc0-db51-43c5-c36e-65e94462f221"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.79)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading langchain_openai-1.0.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading langchain_openai-1.0.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.35-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.3.0-py3-none-any.whl (328 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.9/328.9 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, pypdf, pymupdf, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, dataclasses-json, langchain-openai, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 faiss-cpu-1.13.0 langchain-community-0.3.31 langchain-openai-0.3.35 marshmallow-3.26.1 mypy-extensions-1.1.0 pymupdf-1.26.6 pypdf-6.3.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "4xpNN-qKBXMM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## READ all the pdfs inside the directory\n",
        "### Read all the pdf's inside the directory\n",
        "def process_all_pdfs(pdf_directory):\n",
        "    \"\"\"Process all PDF files in a directory\"\"\"\n",
        "    all_documents = []\n",
        "    pdf_dir = Path(pdf_directory)\n",
        "\n",
        "    # Find all PDF files recursively\n",
        "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
        "        try:\n",
        "            loader = PyPDFLoader(str(pdf_file))\n",
        "            documents = loader.load()\n",
        "\n",
        "            # Add source information to metadata\n",
        "            for doc in documents:\n",
        "                doc.metadata['source_file'] = pdf_file.name\n",
        "                doc.metadata['file_type'] = 'pdf'\n",
        "\n",
        "            all_documents.extend(documents)\n",
        "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error: {e}\")\n",
        "\n",
        "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
        "    return all_documents\n",
        "\n",
        "# Process all PDFs in the data directory\n",
        "all_pdf_documents = process_all_pdfs(\"data\")"
      ],
      "metadata": {
        "id": "vm7UwNDcBwvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9bdecba-6aea-49d5-d933-214dd4ec3dbe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 PDF files to process\n",
            "\n",
            "Processing: Rag.pdf\n",
            "  ✓ Loaded 2 pages\n",
            "\n",
            "Processing: embeding.pdf\n",
            "  ✓ Loaded 1 pages\n",
            "\n",
            "Total documents loaded: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_pdf_documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33OwDceIGgjk",
        "outputId": "ccefc506-b1ac-43e4-b365-f57e58a455be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-11-19T07:41:40+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-11-19T07:41:40+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'data/pdf/Rag.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'Rag.pdf', 'file_type': 'pdf'}, page_content='Page 1 — What is RAG?\\nRetrieval-Augmented Generation (RAG) is a technique that enhances Large Language Models by\\nallowing them to retrieve relevant external knowledge before generating answers.\\nKey Benefits:\\n- Reduces hallucinations\\n- Allows up■to■date information retrieval\\n- Enables domain■specific knowledge grounding\\nRAG Components:\\n1. Document Loader\\n2. Text Splitter\\n3. Embedding Model\\n4. Vector Database\\n5. Retriever\\n6. Generator (LLM)\\nPage 2 — RAG Architecture Workflow\\n1. Ingestion Phase:\\n- Load documents (PDF, Web pages, text files, etc.)\\n- Clean and preprocess text\\n- Split text into smaller chunks\\n- Generate embeddings\\n- Store vectors inside a vector database like FAISS, Pinecone, Weaviate\\n2. Query Phase:\\n- User asks a question\\n- Query is converted into an embedding\\n- Vector database retrieves similar chunks\\n- Retrieved context is passed to the LLM\\n- Model generates grounded answers\\nCommon Use Cases:\\n- Chatbots\\n- Document Q&A;\\n- Enterprise knowledge search\\n- Codebase assistants\\nPage 3 — RAG Best Practices & Common Issues\\nBest Practices:\\n- Maintain small chunk sizes (200–350 tokens)\\n- Use high■quality embedding models\\n- Remove duplicate or noisy text\\n- Index metadata for filtering\\n- Re-rank results for higher accuracy\\nChallenges:\\n- Poor chunking leads to irrelevant retrieval\\n- Outdated vector database content\\n- Embeddings mismatch between query and documents\\n- Long■form retrieval complexity'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-11-19T07:41:40+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-11-19T07:41:40+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'data/pdf/Rag.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'Rag.pdf', 'file_type': 'pdf'}, page_content='This PDF is created for RAG practice, testing loaders, chunkers, and embedding pipelines.'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-11-19T07:41:11+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-11-19T07:41:11+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'data/pdf/embeding.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'embeding.pdf', 'file_type': 'pdf'}, page_content='Page 1 — Introduction\\nThis is a dummy PDF created for practicing Retrieval-Augmented Generation (RAG). It contains\\nmultiple sections and structured text so you can test PDF loading, text extraction, chunking, and\\nembeddings.\\nOverview:\\n- RAG combines retrieval + generation.\\n- PDFs often must be chunked into embeddings.\\n- This dummy document simulates a small knowledge base.\\nPage 2 — Product Information\\nProduct A: SmartVision AI Camera\\nDetails:\\n- Resolution: 4K Ultra HD\\n- Features: Face Detection, Object Tracking, Night Vision\\n- Use Case: Security, Office Automation, Traffic Monitoring\\nProduct B: DataSense Analytics Engine\\n- Real-time dashboards\\n- Forecasting models\\n- API integration support\\nFAQ:\\nQ1: How is data stored?\\nA: Encrypted cloud storage.\\nQ2: Does it support offline mode?\\nA: Yes, local caching is available.\\nPage 3 — Additional Notes & Metadata\\nThis section contains structured bullet points and textual variations to test chunk splitting:\\n- RAG workflow steps:\\n1. Load documents\\n2. Chunk text\\n3. Generate embeddings\\n4. Store in vector database\\n5. Retrieve relevant chunks\\n6. Feed into LLM\\n- Sample Keywords: artificial intelligence, computer vision, analytics, security, embeddings,\\nchunking.\\nEnd of Document.')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from pathlib import Path\n",
        "\n",
        "# Text Spliting get into chunks\n",
        "\n",
        "def split_docs(documents,chunk_size=1000,chunk_overlap=200):\n",
        "  \"\"\"Split documents into chunks\"\"\"\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
        "                                                 chunk_overlap=chunk_overlap,\n",
        "                                                 length_function=len,\n",
        "                                                 separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
        "  documents = text_splitter.split_documents(documents)\n",
        "  print(f\"Split {len(documents)} documents into {len(documents)} chunks\")\n",
        "  if documents:\n",
        "    print(f\"\\nExample chunk:\")\n",
        "    print(f\"Content: {documents[0].page_content[:200]}...\")\n",
        "    print(f\"Metadata: {documents[0].metadata}\")\n",
        "  return documents\n"
      ],
      "metadata": {
        "id": "S4QPD10jGrMS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks=split_docs(all_pdf_documents)\n",
        "chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1phoBuaIEd3",
        "outputId": "fa83f2d3-d92b-4e2e-b649-b040cb63b348"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 5 documents into 5 chunks\n",
            "\n",
            "Example chunk:\n",
            "Content: Page 1 — What is RAG?\n",
            "Retrieval-Augmented Generation (RAG) is a technique that enhances Large Language Models by\n",
            "allowing them to retrieve relevant external knowledge before generating answers.\n",
            "Key Be...\n",
            "Metadata: {'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-11-19T07:41:40+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-11-19T07:41:40+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'data/pdf/Rag.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'Rag.pdf', 'file_type': 'pdf'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-11-19T07:41:40+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-11-19T07:41:40+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'data/pdf/Rag.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'Rag.pdf', 'file_type': 'pdf'}, page_content='Page 1 — What is RAG?\\nRetrieval-Augmented Generation (RAG) is a technique that enhances Large Language Models by\\nallowing them to retrieve relevant external knowledge before generating answers.\\nKey Benefits:\\n- Reduces hallucinations\\n- Allows up■to■date information retrieval\\n- Enables domain■specific knowledge grounding\\nRAG Components:\\n1. Document Loader\\n2. Text Splitter\\n3. Embedding Model\\n4. Vector Database\\n5. Retriever\\n6. Generator (LLM)\\nPage 2 — RAG Architecture Workflow\\n1. Ingestion Phase:\\n- Load documents (PDF, Web pages, text files, etc.)\\n- Clean and preprocess text\\n- Split text into smaller chunks\\n- Generate embeddings\\n- Store vectors inside a vector database like FAISS, Pinecone, Weaviate\\n2. Query Phase:\\n- User asks a question\\n- Query is converted into an embedding\\n- Vector database retrieves similar chunks\\n- Retrieved context is passed to the LLM\\n- Model generates grounded answers\\nCommon Use Cases:\\n- Chatbots\\n- Document Q&A;\\n- Enterprise knowledge search\\n- Codebase assistants'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-11-19T07:41:40+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-11-19T07:41:40+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'data/pdf/Rag.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'Rag.pdf', 'file_type': 'pdf'}, page_content='- Retrieved context is passed to the LLM\\n- Model generates grounded answers\\nCommon Use Cases:\\n- Chatbots\\n- Document Q&A;\\n- Enterprise knowledge search\\n- Codebase assistants\\nPage 3 — RAG Best Practices & Common Issues\\nBest Practices:\\n- Maintain small chunk sizes (200–350 tokens)\\n- Use high■quality embedding models\\n- Remove duplicate or noisy text\\n- Index metadata for filtering\\n- Re-rank results for higher accuracy\\nChallenges:\\n- Poor chunking leads to irrelevant retrieval\\n- Outdated vector database content\\n- Embeddings mismatch between query and documents\\n- Long■form retrieval complexity'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-11-19T07:41:40+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-11-19T07:41:40+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'data/pdf/Rag.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'Rag.pdf', 'file_type': 'pdf'}, page_content='This PDF is created for RAG practice, testing loaders, chunkers, and embedding pipelines.'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-11-19T07:41:11+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-11-19T07:41:11+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'data/pdf/embeding.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'embeding.pdf', 'file_type': 'pdf'}, page_content='Page 1 — Introduction\\nThis is a dummy PDF created for practicing Retrieval-Augmented Generation (RAG). It contains\\nmultiple sections and structured text so you can test PDF loading, text extraction, chunking, and\\nembeddings.\\nOverview:\\n- RAG combines retrieval + generation.\\n- PDFs often must be chunked into embeddings.\\n- This dummy document simulates a small knowledge base.\\nPage 2 — Product Information\\nProduct A: SmartVision AI Camera\\nDetails:\\n- Resolution: 4K Ultra HD\\n- Features: Face Detection, Object Tracking, Night Vision\\n- Use Case: Security, Office Automation, Traffic Monitoring\\nProduct B: DataSense Analytics Engine\\n- Real-time dashboards\\n- Forecasting models\\n- API integration support\\nFAQ:\\nQ1: How is data stored?\\nA: Encrypted cloud storage.\\nQ2: Does it support offline mode?\\nA: Yes, local caching is available.\\nPage 3 — Additional Notes & Metadata\\nThis section contains structured bullet points and textual variations to test chunk splitting:\\n- RAG workflow steps:\\n1. Load documents'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-11-19T07:41:11+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-11-19T07:41:11+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'data/pdf/embeding.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'embeding.pdf', 'file_type': 'pdf'}, page_content='Page 3 — Additional Notes & Metadata\\nThis section contains structured bullet points and textual variations to test chunk splitting:\\n- RAG workflow steps:\\n1. Load documents\\n2. Chunk text\\n3. Generate embeddings\\n4. Store in vector database\\n5. Retrieve relevant chunks\\n6. Feed into LLM\\n- Sample Keywords: artificial intelligence, computer vision, analytics, security, embeddings,\\nchunking.\\nEnd of Document.')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#embedding\n",
        "# Install Sentence Transformers for creating embeddings\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "# Install FAISS-CPU for high-performance vector search (CPU version)\n",
        "# !pip install -q faiss-cpu\n",
        "\n",
        "# Install ChromaDB as your open-source vector store\n",
        "!pip install -q chromadb\n",
        "\n",
        "# # Optionally, install LangChain for easier orchestration (highly recommended for RAG)\n",
        "# !pip install -q langchain\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2BUFwcOqOYN",
        "outputId": "c955e695-d8cc-47da-caa9-a4352ceab480"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import uuid\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "rdDUGHdZroWc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingManager:\n",
        "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        Initialize the embedding manager\n",
        "\n",
        "        Args:\n",
        "            model_name: HuggingFace model name for sentence embeddings\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
        "        try:\n",
        "            print(f\"Loading embedding model: {self.model_name}\")\n",
        "            self.model = SentenceTransformer(self.model_name)\n",
        "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {self.model_name}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate embeddings for a list of texts\n",
        "\n",
        "        Args:\n",
        "            texts: List of text strings to embed\n",
        "\n",
        "        Returns:\n",
        "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
        "        \"\"\"\n",
        "        if not self.model:\n",
        "            raise ValueError(\"Model not loaded\")\n",
        "\n",
        "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
        "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
        "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "## initialize the embedding manager\n",
        "\n",
        "embedding_manager=EmbeddingManager()\n",
        "embedding_manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9BlmANCsmCR",
        "outputId": "ee016240-cb11-4e41-8b6b-e286c8070e28"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model: all-MiniLM-L6-v2\n",
            "Model loaded successfully. Embedding dimension: 384\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.EmbeddingManager at 0x7b6ce73bdfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStore:\n",
        "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
        "\n",
        "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"data/vector_store\"):\n",
        "        \"\"\"\n",
        "        Initialize the vector store\n",
        "\n",
        "        Args:\n",
        "            collection_name: Name of the ChromaDB collection\n",
        "            persist_directory: Directory to persist the vector store\n",
        "        \"\"\"\n",
        "        self.collection_name = collection_name\n",
        "        self.persist_directory = persist_directory\n",
        "        self.client = None\n",
        "        self.collection = None\n",
        "        self._initialize_store()\n",
        "\n",
        "    def _initialize_store(self):\n",
        "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
        "        try:\n",
        "            # Create persistent ChromaDB client\n",
        "            os.makedirs(self.persist_directory, exist_ok=True)\n",
        "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
        "\n",
        "            # Get or create collection\n",
        "            self.collection = self.client.get_or_create_collection(\n",
        "                name=self.collection_name,\n",
        "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
        "            )\n",
        "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
        "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing vector store: {e}\")\n",
        "            raise\n",
        "\n",
        "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
        "        \"\"\"\n",
        "        Add documents and their embeddings to the vector store\n",
        "\n",
        "        Args:\n",
        "            documents: List of LangChain documents\n",
        "            embeddings: Corresponding embeddings for the documents\n",
        "        \"\"\"\n",
        "        if len(documents) != len(embeddings):\n",
        "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
        "\n",
        "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
        "\n",
        "        # Prepare data for ChromaDB\n",
        "        ids = []\n",
        "        metadatas = []\n",
        "        documents_text = []\n",
        "        embeddings_list = []\n",
        "\n",
        "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
        "            # Generate unique ID\n",
        "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
        "            ids.append(doc_id)\n",
        "\n",
        "            # Prepare metadata\n",
        "            metadata = dict(doc.metadata)\n",
        "            metadata['doc_index'] = i\n",
        "            metadata['content_length'] = len(doc.page_content)\n",
        "            metadatas.append(metadata)\n",
        "\n",
        "            # Document content\n",
        "            documents_text.append(doc.page_content)\n",
        "\n",
        "            # Embedding\n",
        "            embeddings_list.append(embedding.tolist())\n",
        "\n",
        "        # Add to collection\n",
        "        try:\n",
        "            self.collection.add(\n",
        "                ids=ids,\n",
        "                embeddings=embeddings_list,\n",
        "                metadatas=metadatas,\n",
        "                documents=documents_text\n",
        "            )\n",
        "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
        "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding documents to vector store: {e}\")\n",
        "            raise\n",
        "\n",
        "vectorstore=VectorStore()\n",
        "vectorstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhndGPk35d5g",
        "outputId": "5d3ce204-3527-4a76-a73f-79a9088a9c2c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store initialized. Collection: pdf_documents\n",
            "Existing documents in collection: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.VectorStore at 0x7b6ce4509100>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAjgKjtY7oHq",
        "outputId": "f73ad032-e0e0-4d94-dd4b-7e8f272b2f42"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-11-19T07:41:40+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-11-19T07:41:40+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'data/pdf/Rag.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'Rag.pdf', 'file_type': 'pdf'}, page_content='Page 1 — What is RAG?\\nRetrieval-Augmented Generation (RAG) is a technique that enhances Large Language Models by\\nallowing them to retrieve relevant external knowledge before generating answers.\\nKey Benefits:\\n- Reduces hallucinations\\n- Allows up■to■date information retrieval\\n- Enables domain■specific knowledge grounding\\nRAG Components:\\n1. Document Loader\\n2. Text Splitter\\n3. Embedding Model\\n4. Vector Database\\n5. Retriever\\n6. Generator (LLM)\\nPage 2 — RAG Architecture Workflow\\n1. Ingestion Phase:\\n- Load documents (PDF, Web pages, text files, etc.)\\n- Clean and preprocess text\\n- Split text into smaller chunks\\n- Generate embeddings\\n- Store vectors inside a vector database like FAISS, Pinecone, Weaviate\\n2. Query Phase:\\n- User asks a question\\n- Query is converted into an embedding\\n- Vector database retrieves similar chunks\\n- Retrieved context is passed to the LLM\\n- Model generates grounded answers\\nCommon Use Cases:\\n- Chatbots\\n- Document Q&A;\\n- Enterprise knowledge search\\n- Codebase assistants'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-11-19T07:41:40+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-11-19T07:41:40+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'data/pdf/Rag.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'Rag.pdf', 'file_type': 'pdf'}, page_content='- Retrieved context is passed to the LLM\\n- Model generates grounded answers\\nCommon Use Cases:\\n- Chatbots\\n- Document Q&A;\\n- Enterprise knowledge search\\n- Codebase assistants\\nPage 3 — RAG Best Practices & Common Issues\\nBest Practices:\\n- Maintain small chunk sizes (200–350 tokens)\\n- Use high■quality embedding models\\n- Remove duplicate or noisy text\\n- Index metadata for filtering\\n- Re-rank results for higher accuracy\\nChallenges:\\n- Poor chunking leads to irrelevant retrieval\\n- Outdated vector database content\\n- Embeddings mismatch between query and documents\\n- Long■form retrieval complexity'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-11-19T07:41:40+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-11-19T07:41:40+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'data/pdf/Rag.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'Rag.pdf', 'file_type': 'pdf'}, page_content='This PDF is created for RAG practice, testing loaders, chunkers, and embedding pipelines.'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-11-19T07:41:11+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-11-19T07:41:11+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'data/pdf/embeding.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'embeding.pdf', 'file_type': 'pdf'}, page_content='Page 1 — Introduction\\nThis is a dummy PDF created for practicing Retrieval-Augmented Generation (RAG). It contains\\nmultiple sections and structured text so you can test PDF loading, text extraction, chunking, and\\nembeddings.\\nOverview:\\n- RAG combines retrieval + generation.\\n- PDFs often must be chunked into embeddings.\\n- This dummy document simulates a small knowledge base.\\nPage 2 — Product Information\\nProduct A: SmartVision AI Camera\\nDetails:\\n- Resolution: 4K Ultra HD\\n- Features: Face Detection, Object Tracking, Night Vision\\n- Use Case: Security, Office Automation, Traffic Monitoring\\nProduct B: DataSense Analytics Engine\\n- Real-time dashboards\\n- Forecasting models\\n- API integration support\\nFAQ:\\nQ1: How is data stored?\\nA: Encrypted cloud storage.\\nQ2: Does it support offline mode?\\nA: Yes, local caching is available.\\nPage 3 — Additional Notes & Metadata\\nThis section contains structured bullet points and textual variations to test chunk splitting:\\n- RAG workflow steps:\\n1. Load documents'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-11-19T07:41:11+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-11-19T07:41:11+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'data/pdf/embeding.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'embeding.pdf', 'file_type': 'pdf'}, page_content='Page 3 — Additional Notes & Metadata\\nThis section contains structured bullet points and textual variations to test chunk splitting:\\n- RAG workflow steps:\\n1. Load documents\\n2. Chunk text\\n3. Generate embeddings\\n4. Store in vector database\\n5. Retrieve relevant chunks\\n6. Feed into LLM\\n- Sample Keywords: artificial intelligence, computer vision, analytics, security, embeddings,\\nchunking.\\nEnd of Document.')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Convert the text to embeddings\n",
        "texts=[doc.page_content for doc in chunks]\n",
        "\n",
        "## Generate the Embeddings\n",
        "\n",
        "embeddings=embedding_manager.generate_embeddings(texts)\n",
        "\n",
        "##store int he vector dtaabase\n",
        "vectorstore.add_documents(chunks,embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "651628fa3d2c4de4bd51429b269f7a91",
            "b3ed6790cc274b998946d7b1b17ed1b4",
            "ae40a0ae54ab477ea5cde185efe2ca2f",
            "69fc1fa2e5b94e358364ec59c4ff9bfa",
            "c6089957b6a64aefa3a82fa57eea2096",
            "f3a841e335fe4c72a13efc47b3f99dcc",
            "02e6771b144e45ad82a9f660ada4d97a",
            "915cf7462dc64dd3a4d5e3239bec0cae",
            "7f733f86a4f54725acbad212c754931a",
            "6251eb0f45604b8bb88b4207fd96226a",
            "e26b8658787043d783cd0a8065f3ae70"
          ]
        },
        "id": "l7Hvsqw975Jx",
        "outputId": "5d5a1a70-dc3c-4d39-a364-776f42730967"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for 5 texts...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "651628fa3d2c4de4bd51429b269f7a91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings with shape: (5, 384)\n",
            "Adding 5 documents to vector store...\n",
            "Successfully added 5 documents to vector store\n",
            "Total documents in collection: 5\n"
          ]
        }
      ]
    }
  ]
}